{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "-aSMgqFANVAn",
        "outputId": "2ad16abb-1917-42dc-9d56-45bf549f5328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/ATNLP\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/ATNLP'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/ATNLP \n",
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB_zc2qrFn_v",
        "outputId": "a815c075-1215-4622-edac-f3a44c045cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9 MB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 54.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 174 kB 42.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 930 kB/s \n",
            "\u001b[K     |████████████████████████████████| 173 kB 45.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 53.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 10.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 38.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 38.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 34.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 48.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 39.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 47.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 46.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 48.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 45.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 48.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 48.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 55.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 53.1 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -Uq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "ev2SdGrc-hSh",
        "outputId": "09d9b3d8-6692-4338-cfbe-cf2bd95f6628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9DjC1kXzbht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c791aa0d-93ed-449d-b109-0a2dbe773c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import spacy\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_value_\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgY9m5ZXAtpe",
        "outputId": "8c9e3a66-5175-479a-8ed0-2b9e01d3cf9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXezmZQSKrkv"
      },
      "source": [
        "### Loading data files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN-ss_dnbZgC"
      },
      "source": [
        "SCAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53BqSY0kSHZb"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Format:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        # self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "            self.word2count[word] = 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9xjAGaJTdLu"
      },
      "outputs": [],
      "source": [
        "def readFile(filename):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('SCAN/%s.txt' % filename, encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "    # exclude \"IN: \"\n",
        "    pairs = [s[4:].split(' OUT: ') for s in lines]\n",
        "\n",
        "    input_lang = Format(\"input\")\n",
        "    output_lang = Format(\"output\")\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlOUI_akrr5e"
      },
      "outputs": [],
      "source": [
        "def prepareData(filename):\n",
        "    input_lang, output_lang, pairs = readFile(filename)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "kx3rASjgblAF",
        "outputId": "83bcd475-81ef-41b0-ed97-33c20e5d1a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f393b2c65bcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"simple_split/tasks_train_simple\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minput_lang_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"simple_split/tasks_test_simple\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-fe81e4901fbc>\u001b[0m in \u001b[0;36mprepareData\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepareData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read %s sentence pairs\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Counting words...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-961bf97ec2c9>\u001b[0m in \u001b[0;36mreadFile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Read the file and split into lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SCAN/%s.txt'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# exclude \"IN: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SCAN/simple_split/tasks_train_simple.txt'"
          ]
        }
      ],
      "source": [
        "input_lang, output_lang, pairs = prepareData(\"simple_split/tasks_train_simple\")\n",
        "input_lang_test, output_lang_test, pairs_test = prepareData(\"simple_split/tasks_test_simple\")\n",
        "print(random.choice(pairs))\n",
        "print(len(pairs), type(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WkwSyy5_L6S"
      },
      "outputs": [],
      "source": [
        "input_lang"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwoqD40QNxAl"
      },
      "source": [
        "### Models （re-implementations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5wn45CLNxLg"
      },
      "source": [
        "##### Overall-best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB1e-fxZhq_3"
      },
      "source": [
        "2-layer LSTM with 200 hidden units per layer, no attention, and dropout applied at the 0.5 level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdGGTAPwSGur"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 emb_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 dropout):\n",
        "      \n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers   \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUPQB59FNTo4"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwT1Ds6ATU9l"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the  tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VxS9NawN_8S"
      },
      "source": [
        "##### Experiment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiZlLbMTe7GC"
      },
      "source": [
        "LSTM with no attention, 2 layers of 200 hidden units, and no dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHTslyR_fIKg"
      },
      "outputs": [],
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_size, \n",
        "                 hidden_size, \n",
        "                 dropout_prob=0.0):\n",
        "      \n",
        "        super(EncoderLSTM, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=2, dropout=dropout_prob)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        # embedded = [1, 1, emb dim]\n",
        "        embedded = (self.dropout(self.embedding(input))).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        \n",
        "        ## hidden is a tuple (hidden, cell)\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(2, 1, self.hidden_size, device=device), \n",
        "                torch.zeros(2, 1, self.hidden_size, device=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THGRSUrnfjny"
      },
      "outputs": [],
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "\n",
        "    # add dropout\n",
        "    def __init__(self, hidden_size, output_size, dropout_prob=0.0):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=2, dropout=dropout_prob)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.lstm(output, hidden)\n",
        "\n",
        "        # ? paramter sharing \n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(2, 1, self.hidden_size, device=device), torch.zeros(2, 1, self.hidden_size, device=device))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM7TF4ieVCNQ"
      },
      "source": [
        "### Training and Evaluation of the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAL5BYHAeFUK"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2SEa5HhVDD2"
      },
      "outputs": [],
      "source": [
        "# obtain word indices in a sentence\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "# transform to tensor format and add a special token\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "# for input and target (one-hot vectors)\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwodNENOimsG"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # put each word into encoder at time t intead of a sentence togther\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    clip_grad_value_(encoder.parameters(), 5)\n",
        "    clip_grad_value_(decoder.parameters(), 5)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGJQVckBipee"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro7oOzNKitC3"
      },
      "outputs": [],
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.001):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    # Note that, in all experiments, the number of distinct training commands is well below 100k: we randomly sampled them with replacement to reach the target size (100,000)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "\n",
        "    # loss function\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        \n",
        "        ### Print the loss and Time\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "            \n",
        "        if iter % plot_every == 0:\n",
        "            wandb.log({\"loss\": plot_loss_total / plot_every})\n",
        "            # plot_loss_avg = plot_loss_total / plot_every\n",
        "            # plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "    # showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrQMQTROa7CM"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_Gzgg3reTec"
      },
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, sentence):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "\n",
        "        while(True):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pCOnJfOeX2d"
      },
      "outputs": [],
      "source": [
        "def accuracy(encoder, decoder, pairs):\n",
        "    sum = 0\n",
        "    i = 0\n",
        "    for pair in pairs:\n",
        "        i+=1\n",
        "        # print(\"%.2f\" % (i/len(pairs))) \n",
        "        output_words = evaluate(encoder, decoder, pair[0])[0:-1]\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        if output_sentence == pair[1]:\n",
        "          sum +=1\n",
        "    wandb.log({\"accuracy\": sum/len(pairs)})\n",
        "    return sum/len(pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHu-snhJVYGT"
      },
      "source": [
        "### Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fixed length\n"
      ],
      "metadata": {
        "id": "VScrEdTSQXRs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "69d0b29594aa4c4cab543290a7076ec8",
            "623ed8ea936b4524a75fec46ed8a1e0e",
            "02b4e67b36ca4133b6d114d295e49a7d",
            "5a8c89809dad426ab8a95fe21bf6e100",
            "cb6fb3d8b61d40e2826456d23aeff774",
            "46eb61d141b0432dbe7c382938bcbf2b",
            "378d9937cbc8497db699fbee8a5fc0f7",
            "9fc0a0d1a70d4f7e97784332daae2652"
          ]
        },
        "id": "vfV4QO5Finim",
        "outputId": "7b3c971d-1b2e-4d8c-e23b-0c989cd16275"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuqinzhou\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/ATNLP/wandb/run-20221213_092626-5de4lpjc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/5de4lpjc\" target=\"_blank\">Exp1_simple_Exp1_3</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training:\n",
            "0m 46s (- 37m 41s) (2000 2%) 1.0767\n",
            "1m 25s (- 34m 2s) (4000 4%) 0.4964\n",
            "2m 3s (- 32m 16s) (6000 6%) 0.3091\n",
            "2m 44s (- 31m 35s) (8000 8%) 0.2175\n",
            "3m 23s (- 30m 29s) (10000 10%) 0.1518\n",
            "4m 2s (- 29m 38s) (12000 12%) 0.1254\n",
            "4m 40s (- 28m 44s) (14000 14%) 0.0875\n",
            "5m 20s (- 28m 0s) (16000 16%) 0.0775\n",
            "5m 59s (- 27m 17s) (18000 18%) 0.0677\n",
            "6m 38s (- 26m 33s) (20000 20%) 0.0456\n",
            "7m 17s (- 25m 52s) (22000 22%) 0.0339\n",
            "7m 57s (- 25m 12s) (24000 24%) 0.0290\n",
            "8m 38s (- 24m 36s) (26000 26%) 0.0295\n",
            "9m 17s (- 23m 54s) (28000 28%) 0.0308\n",
            "9m 57s (- 23m 13s) (30000 30%) 0.0199\n",
            "10m 36s (- 22m 31s) (32000 32%) 0.0239\n",
            "11m 15s (- 21m 52s) (34000 34%) 0.0148\n",
            "11m 54s (- 21m 10s) (36000 36%) 0.0110\n",
            "12m 34s (- 20m 30s) (38000 38%) 0.0111\n",
            "13m 13s (- 19m 50s) (40000 40%) 0.0123\n",
            "13m 54s (- 19m 12s) (42000 42%) 0.0163\n",
            "14m 33s (- 18m 32s) (44000 44%) 0.0121\n",
            "15m 12s (- 17m 51s) (46000 46%) 0.0142\n",
            "15m 52s (- 17m 11s) (48000 48%) 0.0050\n",
            "16m 31s (- 16m 31s) (50000 50%) 0.0130\n",
            "17m 10s (- 15m 50s) (52000 52%) 0.0080\n",
            "17m 48s (- 15m 10s) (54000 54%) 0.0053\n",
            "18m 28s (- 14m 30s) (56000 56%) 0.0075\n",
            "19m 6s (- 13m 50s) (58000 57%) 0.0100\n",
            "19m 49s (- 13m 12s) (60000 60%) 0.0026\n",
            "20m 28s (- 12m 32s) (62000 62%) 0.0087\n",
            "21m 7s (- 11m 52s) (64000 64%) 0.0011\n",
            "21m 45s (- 11m 12s) (66000 66%) 0.0123\n",
            "22m 24s (- 10m 32s) (68000 68%) 0.0043\n",
            "23m 3s (- 9m 53s) (70000 70%) 0.0013\n",
            "23m 42s (- 9m 13s) (72000 72%) 0.0057\n",
            "24m 21s (- 8m 33s) (74000 74%) 0.0017\n",
            "25m 3s (- 7m 54s) (76000 76%) 0.0121\n",
            "25m 43s (- 7m 15s) (78000 78%) 0.0019\n",
            "26m 22s (- 6m 35s) (80000 80%) 0.0078\n",
            "27m 1s (- 5m 55s) (82000 82%) 0.0019\n",
            "27m 40s (- 5m 16s) (84000 84%) 0.0027\n",
            "28m 18s (- 4m 36s) (86000 86%) 0.0055\n",
            "28m 57s (- 3m 56s) (88000 88%) 0.0006\n",
            "29m 36s (- 3m 17s) (90000 90%) 0.0009\n",
            "30m 15s (- 2m 37s) (92000 92%) 0.0052\n",
            "30m 58s (- 1m 58s) (94000 94%) 0.0095\n",
            "31m 37s (- 1m 19s) (96000 96%) 0.0017\n",
            "32m 16s (- 0m 39s) (98000 98%) 0.0001\n",
            "32m 55s (- 0m 0s) (100000 100%) 0.0109\n",
            "Testing:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:5de4lpjc) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69d0b29594aa4c4cab543290a7076ec8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.99904</td></tr><tr><td>loss</td><td>0.01095</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">Exp1_simple_Exp1_3</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/5de4lpjc\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/5de4lpjc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20221213_092626-5de4lpjc/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:5de4lpjc). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/ATNLP/wandb/run-20221213_100000-rrqu7glk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/rrqu7glk\" target=\"_blank\">Exp1_simple_Exp1_4</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training:\n",
            "0m 45s (- 36m 55s) (2000 2%) 1.0686\n",
            "1m 23s (- 33m 12s) (4000 4%) 0.5013\n",
            "2m 1s (- 31m 48s) (6000 6%) 0.3199\n",
            "2m 40s (- 30m 50s) (8000 8%) 0.2065\n",
            "3m 22s (- 30m 22s) (10000 10%) 0.1571\n",
            "4m 2s (- 29m 36s) (12000 12%) 0.1127\n",
            "4m 41s (- 28m 47s) (14000 14%) 0.0732\n",
            "5m 20s (- 28m 1s) (16000 16%) 0.0732\n",
            "5m 59s (- 27m 15s) (18000 18%) 0.0463\n",
            "6m 37s (- 26m 31s) (20000 20%) 0.0409\n",
            "7m 17s (- 25m 50s) (22000 22%) 0.0315\n",
            "7m 56s (- 25m 7s) (24000 24%) 0.0243\n",
            "8m 37s (- 24m 33s) (26000 26%) 0.0201\n",
            "9m 16s (- 23m 52s) (28000 28%) 0.0226\n",
            "9m 55s (- 23m 9s) (30000 30%) 0.0143\n",
            "10m 34s (- 22m 28s) (32000 32%) 0.0109\n",
            "11m 13s (- 21m 47s) (34000 34%) 0.0103\n",
            "11m 52s (- 21m 7s) (36000 36%) 0.0134\n",
            "12m 31s (- 20m 25s) (38000 38%) 0.0118\n",
            "13m 10s (- 19m 45s) (40000 40%) 0.0040\n",
            "13m 50s (- 19m 7s) (42000 42%) 0.0105\n",
            "14m 30s (- 18m 28s) (44000 44%) 0.0079\n",
            "15m 10s (- 17m 48s) (46000 46%) 0.0090\n",
            "15m 49s (- 17m 8s) (48000 48%) 0.0068\n",
            "16m 28s (- 16m 28s) (50000 50%) 0.0057\n",
            "17m 7s (- 15m 48s) (52000 52%) 0.0078\n",
            "17m 46s (- 15m 8s) (54000 54%) 0.0036\n",
            "18m 25s (- 14m 28s) (56000 56%) 0.0030\n",
            "19m 3s (- 13m 48s) (58000 57%) 0.0046\n",
            "19m 45s (- 13m 10s) (60000 60%) 0.0053\n",
            "20m 24s (- 12m 30s) (62000 62%) 0.0012\n",
            "21m 3s (- 11m 50s) (64000 64%) 0.0062\n",
            "21m 42s (- 11m 10s) (66000 66%) 0.0083\n",
            "22m 21s (- 10m 31s) (68000 68%) 0.0024\n",
            "23m 0s (- 9m 51s) (70000 70%) 0.0033\n",
            "23m 40s (- 9m 12s) (72000 72%) 0.0003\n",
            "24m 21s (- 8m 33s) (74000 74%) 0.0122\n",
            "25m 0s (- 7m 53s) (76000 76%) 0.0022\n",
            "25m 39s (- 7m 14s) (78000 78%) 0.0005\n",
            "26m 18s (- 6m 34s) (80000 80%) 0.0060\n",
            "26m 58s (- 5m 55s) (82000 82%) 0.0048\n",
            "27m 37s (- 5m 15s) (84000 84%) 0.0021\n",
            "28m 16s (- 4m 36s) (86000 86%) 0.0004\n",
            "28m 55s (- 3m 56s) (88000 88%) 0.0020\n",
            "29m 33s (- 3m 17s) (90000 90%) 0.0114\n",
            "30m 15s (- 2m 37s) (92000 92%) 0.0002\n",
            "30m 54s (- 1m 58s) (94000 94%) 0.0002\n",
            "31m 33s (- 1m 18s) (96000 96%) 0.0100\n",
            "32m 12s (- 0m 39s) (98000 98%) 0.0014\n",
            "32m 50s (- 0m 0s) (100000 100%) 0.0010\n",
            "Testing:\n"
          ]
        }
      ],
      "source": [
        "for run in range(2):\n",
        "    wandb.init(\n",
        "        project=\"ATNLP\",\n",
        "        name = f\"Exp1_simple_Exp1_{run+3}\",\n",
        "        config={\n",
        "            \"hidden_size\": 200,\n",
        "            \"dropout_prob\": 0,\n",
        "            })\n",
        "    \n",
        "    # Copy your config \n",
        "    config = wandb.config\n",
        "\n",
        "    print(\"Training:\")\n",
        "    exp1_encoder = EncoderLSTM(input_lang.n_words, config.hidden_size, config.dropout_prob).to(device)\n",
        "    exp1_decoder = DecoderLSTM(config.hidden_size, output_lang.n_words).to(device)\n",
        "    trainIters(exp1_encoder, exp1_decoder, 100000, print_every= 2000, plot_every= 2000)\n",
        "\n",
        "    print(\"Testing:\") \n",
        "    accuracy(exp1_encoder, exp1_decoder, pairs_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02b4e67b36ca4133b6d114d295e49a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_378d9937cbc8497db699fbee8a5fc0f7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9fc0a0d1a70d4f7e97784332daae2652",
            "value": 0.06978256829585579
          }
        },
        "378d9937cbc8497db699fbee8a5fc0f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46eb61d141b0432dbe7c382938bcbf2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a8c89809dad426ab8a95fe21bf6e100": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623ed8ea936b4524a75fec46ed8a1e0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb6fb3d8b61d40e2826456d23aeff774",
            "placeholder": "​",
            "style": "IPY_MODEL_46eb61d141b0432dbe7c382938bcbf2b",
            "value": "0.001 MB of 0.010 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "69d0b29594aa4c4cab543290a7076ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_623ed8ea936b4524a75fec46ed8a1e0e",
              "IPY_MODEL_02b4e67b36ca4133b6d114d295e49a7d"
            ],
            "layout": "IPY_MODEL_5a8c89809dad426ab8a95fe21bf6e100"
          }
        },
        "9fc0a0d1a70d4f7e97784332daae2652": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb6fb3d8b61d40e2826456d23aeff774": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}