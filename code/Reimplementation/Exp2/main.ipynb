{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9102a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb45f284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SCAN-master\\\\simple_split/size_variations/tasks_train_simple_p16.txt',\n",
       " 'SCAN-master\\\\simple_split/size_variations/tasks_test_simple_p16.txt')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'SCAN-master'\n",
    "file_name = 'simple_split/size_variations/tasks_train_simple_p16.txt'\n",
    "test_file_name = 'simple_split/size_variations/tasks_test_simple_p16.txt'\n",
    "file_path = os.path.join(dataset_path, file_name)\n",
    "test_file_path = os.path.join(dataset_path, test_file_name)\n",
    "file_path, test_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "980c17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3594978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "command_le = Lang('command')\n",
    "action_le = Lang('action')\n",
    "\n",
    "def dataloader(path):\n",
    "    with open(path, 'r') as f:\n",
    "        dataset = f.readlines()\n",
    "\n",
    "    def preprocess_data(line):\n",
    "        line = line.strip().split()\n",
    "        split_index = line.index('OUT:')\n",
    "        inp = line[1: split_index]\n",
    "        outp = line[split_index+1:]\n",
    "        command_le.addSentence(inp)\n",
    "        action_le.addSentence(outp)\n",
    "        return [inp, outp]\n",
    "    \n",
    "    pairs = list(map(preprocess_data, dataset))\n",
    "    input_commands, output_actions = np.transpose(pairs).tolist()\n",
    "    return input_commands, output_actions, pairs\n",
    "\n",
    "commands, actions, pairs = dataloader(file_path)\n",
    "test_commands, test_acitons, test_pairs = dataloader(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4785c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(le, sentence):\n",
    "    return [le.word2index[word] for word in sentence]\n",
    "\n",
    "def tensorFromSentence(le, sentence):\n",
    "    indexes = indexesFromSentence(le, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1,1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(command_le, pair[0])\n",
    "    target_tensor = tensorFromSentence(action_le, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9d23983a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = max([len(action) for action in actions]) + 1\n",
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3b26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01f581c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    def initCell(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51dc0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, dropout):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # Could be LogSoftmax\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    def initCell(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15bfa840",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion\n",
    "         ):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_cell = encoder.initCell()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    loss = 0\n",
    "    gold_pred = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, (encoder_hidden, encoder_cell) = encoder(\n",
    "            input_tensor[ei], (encoder_hidden, encoder_cell))\n",
    "        \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "    # decoder_cell = decoder.initCell()\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, (decoder_hidden, decoder_cell) = decoder(\n",
    "                decoder_input, (decoder_hidden, decoder_cell))\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            pred = topi.squeeze()\n",
    "            \n",
    "            if torch.equal(pred, target_tensor[di].squeeze()):\n",
    "                gold_pred += 1\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, (decoder_hidden, decoder_cell) = decoder(\n",
    "                decoder_input, (decoder_hidden, decoder_cell))\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            pred = topi.squeeze()\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            if torch.equal(pred, target_tensor[di].squeeze()):\n",
    "                gold_pred += 1\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                target_length = di + 1\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_value_(encoder.parameters(), 5.0)\n",
    "    torch.nn.utils.clip_grad_value_(decoder.parameters(), 5.0)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length, gold_pred, target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4aa434e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12],\n",
       "         [ 3],\n",
       "         [10],\n",
       "         [ 8],\n",
       "         [ 6],\n",
       "         [12],\n",
       "         [ 1]], device='cuda:0'),\n",
       " tensor([[5],\n",
       "         [5],\n",
       "         [4],\n",
       "         [5],\n",
       "         [5],\n",
       "         [4],\n",
       "         [5],\n",
       "         [5],\n",
       "         [4],\n",
       "         [4],\n",
       "         [1]], device='cuda:0'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorsFromPair(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5f9b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b00de19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points, accuracy):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.plot(accuracy)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100,\n",
    "               learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    \n",
    "    plot_losses = []\n",
    "    plot_accs = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    print_pred_total = 0\n",
    "    print_label_total = 0\n",
    "    plot_pred_total = 0\n",
    "    plot_label_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        # target_length = target_tensor.size(0)\n",
    "        \n",
    "        loss, gold_pred, target_length = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_label_total += target_length\n",
    "        print_pred_total += gold_pred\n",
    "        plot_label_total += target_length\n",
    "        plot_pred_total += gold_pred\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_acc_avg = print_pred_total / print_label_total\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_pred_total = 0\n",
    "            print_label_total = 0\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) loss: %.4f acc: %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                iter, iter / n_iters * 100, print_loss_avg, print_acc_avg))\n",
    "            \n",
    "        if iter % plot_every == 0:\n",
    "            plot_acc_avg = plot_pred_total / plot_label_total\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_accs.append(plot_acc_avg)\n",
    "            plot_loss_total = 0\n",
    "            plot_pred_total = 0\n",
    "            plot_label_total = 0\n",
    "            \n",
    "    showPlot(plot_losses, plot_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fcfdcfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, pair, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor, target_tensor = tensorsFromPair(pair)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_cell = encoder.initCell()\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, (encoder_hidden, encoder_cell) = encoder(input_tensor[ei],\n",
    "                                                    (encoder_hidden, encoder_cell))\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_cell = encoder_cell\n",
    "        \n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, (decoder_hidden, decoder_cell) = decoder(\n",
    "                decoder_input, (decoder_hidden, decoder_cell))\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(action_le.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cdbe19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(encoder, decoder, test_pairs):\n",
    "    gold = 0\n",
    "    total_target = 0\n",
    "    for pair in tqdm(test_pairs):\n",
    "        preds = evaluate(encoder, decoder, pair)[:-1]\n",
    "        target_output = pair[1]\n",
    "        length = min(len(preds), len(target_output))\n",
    "        target_length = len(target_output)\n",
    "        total_target += length\n",
    "        \n",
    "        for i in range(length):\n",
    "            if preds[i] == target_output[i]:\n",
    "                gold += 1\n",
    "    return gold / total_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7104852d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 17565/17565 [02:03<00:00, 142.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999085683392035"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(encoder1, decoder1, test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4681b3fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 32s (- 54m 12s) (1000 1%) loss: 1.3601 acc: 0.5374\n",
      "0m 55s (- 45m 1s) (2000 2%) loss: 0.7826 acc: 0.7250\n",
      "1m 16s (- 41m 23s) (3000 3%) loss: 0.5323 acc: 0.8174\n",
      "1m 39s (- 39m 37s) (4000 4%) loss: 0.4138 acc: 0.8489\n",
      "2m 1s (- 38m 21s) (5000 5%) loss: 0.3445 acc: 0.8749\n",
      "2m 23s (- 37m 28s) (6000 6%) loss: 0.2635 acc: 0.9020\n",
      "2m 45s (- 36m 40s) (7000 7%) loss: 0.2271 acc: 0.9131\n",
      "3m 7s (- 35m 57s) (8000 8%) loss: 0.1949 acc: 0.9208\n",
      "3m 29s (- 35m 21s) (9000 9%) loss: 0.1639 acc: 0.9334\n",
      "3m 52s (- 34m 52s) (10000 10%) loss: 0.1369 acc: 0.9450\n",
      "4m 14s (- 34m 18s) (11000 11%) loss: 0.1120 acc: 0.9531\n",
      "4m 36s (- 33m 45s) (12000 12%) loss: 0.0992 acc: 0.9597\n",
      "4m 58s (- 33m 19s) (13000 13%) loss: 0.0880 acc: 0.9646\n",
      "5m 20s (- 32m 49s) (14000 14%) loss: 0.0806 acc: 0.9681\n",
      "5m 42s (- 32m 22s) (15000 15%) loss: 0.0701 acc: 0.9691\n",
      "6m 5s (- 31m 57s) (16000 16%) loss: 0.0575 acc: 0.9753\n",
      "6m 27s (- 31m 31s) (17000 17%) loss: 0.0541 acc: 0.9752\n",
      "6m 49s (- 31m 6s) (18000 18%) loss: 0.0514 acc: 0.9786\n",
      "7m 11s (- 30m 39s) (19000 19%) loss: 0.0416 acc: 0.9811\n",
      "7m 33s (- 30m 13s) (20000 20%) loss: 0.0275 acc: 0.9848\n",
      "7m 55s (- 29m 49s) (21000 21%) loss: 0.0350 acc: 0.9841\n",
      "8m 18s (- 29m 27s) (22000 22%) loss: 0.0283 acc: 0.9872\n",
      "8m 40s (- 29m 3s) (23000 23%) loss: 0.0331 acc: 0.9877\n",
      "9m 3s (- 28m 40s) (24000 24%) loss: 0.0211 acc: 0.9901\n",
      "9m 26s (- 28m 18s) (25000 25%) loss: 0.0265 acc: 0.9886\n",
      "9m 48s (- 27m 55s) (26000 26%) loss: 0.0303 acc: 0.9876\n",
      "10m 10s (- 27m 31s) (27000 27%) loss: 0.0136 acc: 0.9936\n",
      "10m 32s (- 27m 6s) (28000 28%) loss: 0.0309 acc: 0.9888\n",
      "10m 54s (- 26m 42s) (29000 28%) loss: 0.0228 acc: 0.9915\n",
      "11m 17s (- 26m 21s) (30000 30%) loss: 0.0196 acc: 0.9907\n",
      "11m 39s (- 25m 57s) (31000 31%) loss: 0.0199 acc: 0.9930\n",
      "12m 2s (- 25m 35s) (32000 32%) loss: 0.0162 acc: 0.9932\n",
      "12m 24s (- 25m 12s) (33000 33%) loss: 0.0095 acc: 0.9955\n",
      "12m 47s (- 24m 49s) (34000 34%) loss: 0.0128 acc: 0.9943\n",
      "13m 9s (- 24m 26s) (35000 35%) loss: 0.0164 acc: 0.9940\n",
      "13m 31s (- 24m 3s) (36000 36%) loss: 0.0104 acc: 0.9950\n",
      "13m 53s (- 23m 39s) (37000 37%) loss: 0.0043 acc: 0.9993\n",
      "14m 16s (- 23m 17s) (38000 38%) loss: 0.0055 acc: 0.9983\n",
      "14m 39s (- 22m 54s) (39000 39%) loss: 0.0087 acc: 0.9970\n",
      "15m 1s (- 22m 31s) (40000 40%) loss: 0.0100 acc: 0.9967\n",
      "15m 22s (- 22m 8s) (41000 41%) loss: 0.0059 acc: 0.9989\n",
      "15m 45s (- 21m 45s) (42000 42%) loss: 0.0048 acc: 0.9980\n",
      "16m 8s (- 21m 24s) (43000 43%) loss: 0.0141 acc: 0.9962\n",
      "16m 31s (- 21m 2s) (44000 44%) loss: 0.0115 acc: 0.9979\n",
      "16m 54s (- 20m 39s) (45000 45%) loss: 0.0016 acc: 0.9997\n",
      "17m 17s (- 20m 17s) (46000 46%) loss: 0.0082 acc: 0.9981\n",
      "17m 39s (- 19m 55s) (47000 47%) loss: 0.0022 acc: 0.9992\n",
      "18m 2s (- 19m 32s) (48000 48%) loss: 0.0186 acc: 0.9944\n",
      "18m 24s (- 19m 9s) (49000 49%) loss: 0.0047 acc: 0.9991\n",
      "18m 47s (- 18m 47s) (50000 50%) loss: 0.0018 acc: 0.9993\n",
      "19m 8s (- 18m 23s) (51000 51%) loss: 0.0041 acc: 0.9990\n",
      "19m 30s (- 18m 0s) (52000 52%) loss: 0.0053 acc: 0.9987\n",
      "19m 53s (- 17m 38s) (53000 53%) loss: 0.0124 acc: 0.9986\n",
      "20m 16s (- 17m 16s) (54000 54%) loss: 0.0017 acc: 0.9998\n",
      "20m 38s (- 16m 53s) (55000 55%) loss: 0.0134 acc: 0.9965\n",
      "21m 1s (- 16m 31s) (56000 56%) loss: 0.0067 acc: 0.9986\n",
      "21m 24s (- 16m 8s) (57000 56%) loss: 0.0039 acc: 0.9985\n",
      "21m 46s (- 15m 46s) (58000 57%) loss: 0.0010 acc: 0.9998\n",
      "22m 9s (- 15m 23s) (59000 59%) loss: 0.0014 acc: 0.9995\n",
      "22m 31s (- 15m 1s) (60000 60%) loss: 0.0035 acc: 0.9988\n",
      "22m 54s (- 14m 38s) (61000 61%) loss: 0.0076 acc: 0.9983\n",
      "23m 16s (- 14m 16s) (62000 62%) loss: 0.0025 acc: 0.9991\n",
      "23m 39s (- 13m 53s) (63000 63%) loss: 0.0134 acc: 0.9966\n",
      "24m 1s (- 13m 30s) (64000 64%) loss: 0.0010 acc: 0.9997\n",
      "24m 23s (- 13m 8s) (65000 65%) loss: 0.0002 acc: 0.9999\n",
      "24m 45s (- 12m 45s) (66000 66%) loss: 0.0056 acc: 0.9990\n",
      "25m 8s (- 12m 22s) (67000 67%) loss: 0.0015 acc: 0.9998\n",
      "25m 30s (- 12m 0s) (68000 68%) loss: 0.0002 acc: 1.0000\n",
      "25m 53s (- 11m 37s) (69000 69%) loss: 0.0032 acc: 0.9994\n",
      "26m 15s (- 11m 15s) (70000 70%) loss: 0.0044 acc: 0.9988\n",
      "26m 37s (- 10m 52s) (71000 71%) loss: 0.0030 acc: 0.9991\n",
      "27m 0s (- 10m 30s) (72000 72%) loss: 0.0026 acc: 0.9990\n",
      "27m 22s (- 10m 7s) (73000 73%) loss: 0.0058 acc: 0.9988\n",
      "27m 45s (- 9m 45s) (74000 74%) loss: 0.0100 acc: 0.9979\n",
      "28m 7s (- 9m 22s) (75000 75%) loss: 0.0022 acc: 0.9992\n",
      "28m 30s (- 9m 0s) (76000 76%) loss: 0.0022 acc: 0.9995\n",
      "28m 51s (- 8m 37s) (77000 77%) loss: 0.0012 acc: 0.9998\n",
      "29m 15s (- 8m 15s) (78000 78%) loss: 0.0231 acc: 0.9982\n",
      "29m 37s (- 7m 52s) (79000 79%) loss: 0.0050 acc: 0.9988\n",
      "30m 0s (- 7m 30s) (80000 80%) loss: 0.0050 acc: 0.9981\n",
      "30m 23s (- 7m 7s) (81000 81%) loss: 0.0008 acc: 0.9999\n",
      "30m 45s (- 6m 45s) (82000 82%) loss: 0.0001 acc: 1.0000\n",
      "31m 8s (- 6m 22s) (83000 83%) loss: 0.0005 acc: 0.9999\n",
      "31m 30s (- 6m 0s) (84000 84%) loss: 0.0001 acc: 1.0000\n",
      "31m 53s (- 5m 37s) (85000 85%) loss: 0.0000 acc: 1.0000\n",
      "32m 16s (- 5m 15s) (86000 86%) loss: 0.0000 acc: 1.0000\n",
      "32m 38s (- 4m 52s) (87000 87%) loss: 0.0000 acc: 1.0000\n",
      "33m 2s (- 4m 30s) (88000 88%) loss: 0.0000 acc: 1.0000\n",
      "33m 25s (- 4m 7s) (89000 89%) loss: 0.0258 acc: 0.9928\n",
      "33m 47s (- 3m 45s) (90000 90%) loss: 0.0041 acc: 0.9993\n",
      "34m 10s (- 3m 22s) (91000 91%) loss: 0.0021 acc: 0.9992\n",
      "34m 32s (- 3m 0s) (92000 92%) loss: 0.0067 acc: 0.9983\n",
      "34m 55s (- 2m 37s) (93000 93%) loss: 0.0028 acc: 0.9993\n",
      "35m 17s (- 2m 15s) (94000 94%) loss: 0.0002 acc: 1.0000\n",
      "35m 39s (- 1m 52s) (95000 95%) loss: 0.0002 acc: 1.0000\n",
      "36m 2s (- 1m 30s) (96000 96%) loss: 0.0032 acc: 0.9990\n",
      "36m 25s (- 1m 7s) (97000 97%) loss: 0.0021 acc: 0.9992\n",
      "36m 47s (- 0m 45s) (98000 98%) loss: 0.0044 acc: 0.9992\n",
      "37m 9s (- 0m 22s) (99000 99%) loss: 0.0009 acc: 0.9997\n",
      "37m 32s (- 0m 0s) (100000 100%) loss: 0.0002 acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hidden_size = 200\n",
    "input_size = command_le.n_words\n",
    "output_size = action_le.n_words\n",
    "encoder1 = EncoderRNN(input_size, hidden_size, num_layers=2, dropout=0).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, output_size, num_layers=2, dropout=0).to(device)\n",
    "\n",
    "trainIters(encoder1, decoder1, 100000, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e41709e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7afb7836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['walk', 'opposite', 'right', 'thrice', 'and', 'run', 'twice'], ['I_TURN_RIGHT', 'I_TURN_RIGHT', 'I_WALK', 'I_TURN_RIGHT', 'I_TURN_RIGHT', 'I_WALK', 'I_TURN_RIGHT', 'I_TURN_RIGHT', 'I_WALK', 'I_RUN', 'I_RUN', '<EOS>']]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<EOS>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pairs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(encoder, decoder, test_pairs)\u001b[0m\n\u001b[0;32m      6\u001b[0m target_output\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<EOS>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(pair)\n\u001b[1;32m----> 8\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(preds), \u001b[38;5;28mlen\u001b[39m(target_output))\n\u001b[0;32m     10\u001b[0m target_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(target_output)\n",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(encoder, decoder, pair, max_length)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(encoder, decoder, pair, max_length\u001b[38;5;241m=\u001b[39mMAX_LENGTH):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 3\u001b[0m         input_tensor, target_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensorsFromPair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m         input_length \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m         encoder_hidden \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39minitHidden()\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mtensorsFromPair\u001b[1;34m(pair)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensorsFromPair\u001b[39m(pair):\n\u001b[0;32m     10\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m tensorFromSentence(command_le, pair[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 11\u001b[0m     target_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensorFromSentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_le\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (input_tensor, target_tensor)\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mtensorFromSentence\u001b[1;34m(le, sentence)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensorFromSentence\u001b[39m(le, sentence):\n\u001b[1;32m----> 5\u001b[0m     indexes \u001b[38;5;241m=\u001b[39m \u001b[43mindexesFromSentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     indexes\u001b[38;5;241m.\u001b[39mappend(EOS_token)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(indexes, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mindexesFromSentence\u001b[1;34m(le, sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindexesFromSentence\u001b[39m(le, sentence):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [le\u001b[38;5;241m.\u001b[39mword2index[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence]\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindexesFromSentence\u001b[39m(le, sentence):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword2index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence]\n",
      "\u001b[1;31mKeyError\u001b[0m: '<EOS>'"
     ]
    }
   ],
   "source": [
    "evaluate_model(encoder1, decoder1, test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07ca326b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['turn', 'opposite', 'left', 'thrice', 'after', 'walk', 'around', 'left'],\n",
       " ['I_TURN_LEFT',\n",
       "  'I_WALK',\n",
       "  'I_TURN_LEFT',\n",
       "  'I_WALK',\n",
       "  'I_TURN_LEFT',\n",
       "  'I_WALK',\n",
       "  'I_TURN_LEFT',\n",
       "  'I_WALK',\n",
       "  'I_TURN_LEFT',\n",
       "  'I_TURN_LEFT',\n",
       "  'I_TURN_LEFT',\n",
       "  'I_TURN_LEFT',\n",
       "  'I_TURN_LEFT',\n",
       "  'I_TURN_LEFT']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72006df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714292b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(EncoderGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a10070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderGRU(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.5):\n",
    "        super(AttnDecoderGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.Ua = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.Wa = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.va = torch.tensor(torch.randn(1, hidden_size), requires_grad=True).cuda()\n",
    "        \n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size * 2, self.output_size)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_hiddens):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        encoder_hiddens = encoder_hiddens.unsqueeze(1)\n",
    "        attn_weights = F.softmax(torch.inner(\n",
    "            self.va, self.tanh(self.Ua(encoder_hiddens) + self.Wa(hidden))), dim=1)\n",
    "        \n",
    "        context = torch.sum(\n",
    "            torch.mul(attn_weights, encoder_hiddens.squeeze()), dim=1)\n",
    "\n",
    "        output = torch.cat((embedded[0], context), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        cat_output = torch.cat((context, hidden[0]), 1)\n",
    "        output = F.log_softmax(self.out(cat_output), dim=1)\n",
    "\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairtensor = tensorsFromPair(pairs[0])\n",
    "input_tensor = pairtensor[0]\n",
    "target_tensor = pairtensor[1]\n",
    "input_size = len(command_le.classes_)\n",
    "output_size = len(action_le.classes_)\n",
    "hidden_size = 200\n",
    "hidden = torch.zeros(1,1,hidden_size, device=device)\n",
    "\n",
    "embedding = nn.Embedding(input_size, hidden_size).cuda()\n",
    "gru = nn.GRU(hidden_size, hidden_size).cuda()\n",
    "embedded = embedding(input_tensor)\n",
    "output = embedded\n",
    "output, hidden = gru(output, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4882ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = torch.randn(9,1,200).cuda()\n",
    "hidden = torch.randn(1,1,200).cuda()\n",
    "tanh = nn.Tanh()\n",
    "Ua = nn.Linear(200, 200).cuda()\n",
    "Wa = nn.Linear(200, 200).cuda()\n",
    "va = torch.tensor(torch.randn(1, 200)).cuda()\n",
    "#weights = torch.inner(Ua, hiddens).squeeze() + torch.inner(Wa, hidden).squeeze()\n",
    "weights = F.softmax(torch.inner(va, tanh(Ua(hiddens)+Wa(hidden))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.sum(\n",
    "            torch.mul(weights, hiddens.squeeze()), dim=1)\n",
    "context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train2(input_tensor, target_tensor, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion\n",
    "          ):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_hiddens = torch.zeros(input_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "    loss = 0\n",
    "    gold_pred = 0\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_hiddens[ei] = encoder_hidden[0, 0]\n",
    "        \n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_hiddens)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            pred = topi.squeeze()\n",
    "            \n",
    "            if torch.equal(pred, target_tensor[di].squeeze()):\n",
    "                gold_pred += 1\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_hiddens)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            pred = topi.squeeze()\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            if torch.equal(pred, target_tensor[di].squeeze()):\n",
    "                gold_pred += 1\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                target_length = di + 1\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=5.0)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm=5.0)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length, gold_pred, target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b418fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters2(encoder, decoder, n_iters, print_every=1000, plot_every=100,\n",
    "               learning_rate=0.001):\n",
    "    start = time.time()\n",
    "    \n",
    "    plot_losses = []\n",
    "    plot_accs = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0\n",
    "    print_pred_total = 0\n",
    "    print_label_total = 0\n",
    "    plot_pred_total = 0\n",
    "    plot_label_total = 0\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        # target_length = target_tensor.size(0)\n",
    "        \n",
    "        loss, gold_pred, target_length = train2(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_label_total += target_length\n",
    "        print_pred_total += gold_pred\n",
    "        plot_label_total += target_length\n",
    "        plot_pred_total += gold_pred\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_acc_avg = print_pred_total / print_label_total\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_pred_total = 0\n",
    "            print_label_total = 0\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) loss: %.4f acc: %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                iter, iter / n_iters * 100, print_loss_avg, print_acc_avg))\n",
    "            \n",
    "        if iter % plot_every == 0:\n",
    "            plot_acc_avg = plot_pred_total / plot_label_total\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_accs.append(plot_acc_avg)\n",
    "            plot_loss_total = 0\n",
    "            plot_pred_total = 0\n",
    "            plot_label_total = 0\n",
    "            \n",
    "    showPlot(plot_losses, plot_accs)\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 50\n",
    "input_size = len(command_le.classes_)\n",
    "output_size = len(action_le.classes_)\n",
    "encoder1 = EncoderGRU(input_size, hidden_size, num_layers=1, dropout=0.5).to(device)\n",
    "decoder1 = AttnDecoderGRU(hidden_size, output_size, dropout_p=0.5).to(device)\n",
    "\n",
    "encoder1, decoder1 = trainIters2(encoder1, decoder1, 10000, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b466203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, pair, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor, target_tensor = tensorsFromPair(pair)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        encoder_hiddens = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            \n",
    "            encoder_hiddens[ei] = encoder_hidden[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_hiddens)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(action_le.transform(topi.item()))\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dfc9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(encoder1, decoder1, pairs[2122])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor = tensorsFromPair(pairs[123])\n",
    "input_length = input_tensor.size()[0]\n",
    "encoder_hidden = encoder1.initHidden()\n",
    "\n",
    "encoder_hiddens = torch.zeros(10, encoder1.hidden_size, device=device)\n",
    "\n",
    "for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder1(input_tensor[ei], encoder_hidden)\n",
    "\n",
    "    encoder_hiddens[ei] = encoder_hidden[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "decoder_hidden = encoder_hidden\n",
    "\n",
    "decoded_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e013c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output, decoder_hidden, decoder_attention = decoder1(\n",
    "                decoder_input, decoder_hidden, encoder_hiddens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a167eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topv, topi = decoder_output.data.topk(1)\n",
    "topv, topi, decoder_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
