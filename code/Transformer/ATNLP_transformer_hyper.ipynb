{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1358,"status":"ok","timestamp":1671619856627,"user":{"displayName":"Yuqin Zhou","userId":"18351214922641694379"},"user_tz":-60},"id":"wGbaf9jAi-1x"},"outputs":[],"source":["import os\n","import json\n","import numpy as np\n","import random\n","from collections import Counter, defaultdict\n","from tqdm import tqdm\n","import time\n","import math\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","import pandas as pd\n","plt.style.use('seaborn-whitegrid')\n","plt.rcParams['figure.dpi'] = 300\n","\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from models.transformer_new import *"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1671619856861,"user":{"displayName":"Yuqin Zhou","userId":"18351214922641694379"},"user_tz":-60},"id":"oTYJj0uvRd8H"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuqinzhou\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import wandb\n","wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"JuQ0ndMKr5GA"},"source":["### Creat Data"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1671619901967,"user":{"displayName":"Yuqin Zhou","userId":"18351214922641694379"},"user_tz":-60},"id":"Bjk00nLMDpZN"},"outputs":[],"source":["SOS_token = 0\n","EOS_token = 1\n","pad_idx = 2\n","\n","class Format:\n","    def __init__(self, name):\n","        self.name = name\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {SOS_token: \"SOS\", EOS_token: \"EOS\", pad_idx: \"pad\"}\n","        self.n_words = 3  \n","\n","    def addSentence(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","            self.word2count[word] = 1\n","        else:\n","            self.word2count[word] += 1\n","\n","\n","def readFile(filename):\n","    print(\"Reading lines...\")\n","\n","    # Read the file and split into lines\n","    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n","\n","    # exclude \"IN: \"\n","    pairs = [s[4:].split(' OUT: ') for s in lines]\n","\n","    input_lang = Format(\"input\")\n","    output_lang = Format(\"output\")\n","\n","    return input_lang, output_lang, pairs\n","  \n","def prepareData(filename):\n","    input_lang, output_lang, pairs = readFile(filename)\n","    print(\"Read %s sentence pairs\" % len(pairs))\n","    print(\"Counting words...\")\n","    for pair in pairs:\n","        input_lang.addSentence(pair[0])\n","        output_lang.addSentence(pair[1])\n","    print(\"Counted words:\")\n","    print(input_lang.name, input_lang.n_words)\n","    print(output_lang.name, output_lang.n_words)\n","    return input_lang, output_lang, pairs\n","\n","\n","# obtain word indices in a sentence\n","def indexesFromSentence(lang, sentence):\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","\n","# transform to tensor format and add a special token\n","def tensorFromSentence_input(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    # indexes.insert(0, SOS_token)\n","    # indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","def tensorFromSentence_output(lang, sentence):\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.insert(0, SOS_token)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","# for input and target (one-hot vectors)\n","def tensorsFromPair(pair):\n","    input_tensor = tensorFromSentence_input(input_lang, pair[0])\n","    target_tensor = tensorFromSentence_output(output_lang, pair[1])\n","    return (input_tensor, target_tensor)\n","\n","\n","def calculate_mean_std(acc_dict):\n","    mean = []\n","    error = []\n","    keys = sorted(acc_dict[0])\n","    num_runs = len(acc_dict)\n","    \n","    for key in keys:\n","        t = []\n","        for d in acc_dict:\n","            t.append(d[key])\n","        mean.append(np.mean(t))\n","        error.append(np.std(t) / np.sqrt(num_runs))\n","    return np.array(mean), np.array(error), keys\n","\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["'/Users/zhouyuqin/Desktop/ATNLP/transformer_scan'"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["%pwd"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading lines...\n","Read 16990 sentence pairs\n","Counting words...\n","Counted words:\n","input 16\n","output 9\n","Reading lines...\n","Read 3920 sentence pairs\n","Counting words...\n","Counted words:\n","input 16\n","output 9\n","['turn opposite right twice and look opposite right twice', 'I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_TURN_RIGHT I_LOOK I_TURN_RIGHT I_TURN_RIGHT I_LOOK']\n","16990 <class 'list'>\n"]}],"source":["# input_lang, output_lang, pairs = prepareData(\"./data/scan/simple_split/tasks_train_simple.txt\")\n","# input_lang_test, output_lang_test, pairs_test = prepareData(\"./data/scan/simple_split/tasks_test_simple.txt\")\n","\n","# input_lang, output_lang, pairs = prepareData(\"./data/scan/simple_split/size_variations/tasks_train_simple_p16.txt\")\n","# input_lang_test, output_lang_test, pairs_test = prepareData(\"./data/scan/simple_split/size_variations/tasks_test_simple_p16.txt\")\n","\n","\n","# input_lang, output_lang, pairs = prepareData(\"/Users/zhouyuqin/Desktop/ATNLP/SCAN/add_prim_split/tasks_train_addprim_jump.txt\")\n","# input_lang_test, output_lang_test, pairs_test = prepareData(\"/Users/zhouyuqin/Desktop/ATNLP/SCAN/add_prim_split/tasks_test_addprim_jump.txt\")\n","\n","\n","\n","input_lang, output_lang, pairs = prepareData(\"/Users/zhouyuqin/Desktop/ATNLP/SCAN/length_split/tasks_train_length.txt\")\n","input_lang_test, output_lang_test, pairs_test = prepareData(\"/Users/zhouyuqin/Desktop/ATNLP/SCAN/length_split/tasks_test_length.txt\")\n","\n","\n","\n","print(random.choice(pairs))\n","print(len(pairs), type(pairs))"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["training_pairs = [tensorsFromPair(pair) for pair in pairs]\n","test_pairs = [tensorsFromPair(pair) for pair in pairs_test]"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["batch = training_pairs[4]"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["run "]}],"source":["for i in batch[0]:\n","    print(input_lang.index2word[i.item()], end = ' ')"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["SOS I_RUN EOS "]}],"source":["for i in batch[1]:\n","    print(output_lang.index2word[i.item()], end = ' ')"]},{"cell_type":"markdown","metadata":{"id":"Erb2TWYZ54eb"},"source":["### Hyperparameters"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":240,"status":"ok","timestamp":1671619936584,"user":{"displayName":"Yuqin Zhou","userId":"18351214922641694379"},"user_tz":-60},"id":"7nyzxL10ZNU3"},"outputs":[],"source":["class Lang:\n","  ''' \n","   ### Training and Data ###\n","    num_runs: Number of runs to do.\n","    num_epochs: Number of training epochs\n","    \n","    ### Models ### \n","    d_model: Dimension of inputs/outputs in transformer\n","    nhead: Number of heads in transformer multihead attention\n","    num_encoder_layers: Number of layers in transformer encoder\n","    num_decoder_layers: Number of layers in transformer decoder\n","    dim_feedforward: Dimension of feedforward layers in transformer\n","    dropout: Dropout rate\n","              \n","    ### Optimization ### \n","    learning_rate: Fixed learning rate for Adam optimizer\n","\n","\n","    ### Output options ###\n","    results_dir: Results subdirectory to save results\n","    out_data_file: Name of output data file with training loss data\n","    checkpoint_path: Path to output saved weights.\n","    checkpoint_every: Epochs before evaluating model and saving weights\n","    record_loss_every: iters before printing and recording loss\n","  '''\n","\n","  def __init__(self, num_runs = 1, num_epochs = 2, d_model = 32, nhead = 8, num_decoder_layers = 2, \n","               num_encoder_layers = 2,  d_feedforward = 256, learning_rate = 0.0005,\n","               dropout = 0.1, checkpoint_every = 1, record_loss_every = 1000, clip = 5, teacher_forcing_ratio = 0):\n","    \n","    self.num_runs = num_runs\n","    self.num_epochs = num_epochs\n","    self.d_model = d_model\n","    self.nhead = nhead\n","    self.num_encoder_layers = num_encoder_layers\n","    self.num_decoder_layers = num_decoder_layers\n","    self.d_feedforward = d_feedforward\n","    self.learning_rate = learning_rate\n","    self.dropout = dropout\n","    self.checkpoint_every = checkpoint_every\n","    self.record_loss_every = record_loss_every\n","    self.clip = clip\n","    self.teacher_forcing_ratio = teacher_forcing_ratio"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671619937825,"user":{"displayName":"Yuqin Zhou","userId":"18351214922641694379"},"user_tz":-60},"id":"WFO9qp3pZfZl"},"outputs":[],"source":["args = Lang()"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["def train(args):\n","    model = Seq2Seq(Encoder(input_lang.n_words, \n","                        args.d_model, \n","                        args.num_encoder_layers, \n","                        args.nhead, \n","                        args.d_feedforward, \n","                        args.dropout, \n","                        device), \n","                Decoder(output_lang.n_words, \n","                        args.d_model, \n","                        args.num_decoder_layers, \n","                        args.nhead, \n","                        args.d_feedforward, \n","                        args.dropout, \n","                        device), pad_idx, pad_idx, device).to(device)\n","                        \n","    model.train()\n","    optimizer = torch.optim.Adam(model.parameters(), lr = args.learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for epoch in range(args.num_epochs):\n","        start_time = time.time()\n","        epoch_loss = 0\n","        print_loss_total = 0 \n","\n","        for iter, batch in enumerate(training_pairs):\n","            loss = 0\n","            optimizer.zero_grad()\n","\n","            ### Encoder ###\n","            src = batch[0].T\n","            src_mask = model.make_src_mask(src)\n","            enc_src = model.encoder(src, src_mask)\n","            \n","            ### Decoder ### \n","            trg = batch[1].T\n","            trg_in = trg[:,:-1] ##[<SOS>, y_1, ..., y_2] \n","            trg_out = trg[:,1:] ##[y_1,..., y_2, <EOS>]\n","            \n","            use_teacher_forcing = True if random.random() < args.teacher_forcing_ratio else False\n","            \n","            if not use_teacher_forcing:\n","                trg_mask = model.make_trg_mask(trg_in)\n","                output, _ = model.decoder(trg_in, enc_src, trg_mask, src_mask)       \n","                loss = criterion(output[0], trg_out.view(-1))    \n","                trg_indexes =  output[0].argmax(1)\n","            \n","            else:\n","                trg_indexes = [SOS_token]\n","                for i in range(trg_in.shape[1]):\n","                    trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n","                    trg_mask = model.make_trg_mask(trg_tensor)\n","                    output, _ = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n","                    # only consider the prediction at time t \n","                    output = output[0][i]\n","                    \n","                    ### gready search\n","                    pred_token = output.argmax(0).item()\n","                    trg_indexes.append(pred_token)\n","                    \n","                    ## calculate loss\n","                    loss += criterion(output, trg_out[0][i])\n","\n","                    # break if the new term is <EOS> \n","                    if pred_token == EOS_token:\n","                        break\n","\n","                trg_indexes = trg_indexes[1:]\n","                loss = loss / trg_in.shape[1]\n","\n","            \n","            loss.backward()\n","            wandb.log({\"train/loss\": loss.data.item(), \"train/iter\": iter})\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","            optimizer.step()\n","            \n","            # Record loss\n","            print_loss_total += loss.data.item()\n","            epoch_loss += loss.data.item()\n","            if (iter + 1) % args.record_loss_every == 0:\n","                print_loss_avg = print_loss_total / args.record_loss_every\n","                print(f'Epoch: {epoch} | Iter: {iter} | Loss: {print_loss_avg:.3f} \\n Target {trg_out[0]} \\n Predict {trg_indexes} type: {use_teacher_forcing}') \n","                print_loss_total = 0 \n","       \n","        ### Recode loss\n","        end_time = time.time()\n","        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","        epoch_loss_avg = epoch_loss / len(training_pairs)\n","\n","        wandb.log({\"train/epoch\": epoch, \"train/epoch_loss\": epoch_loss_avg})\n","        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'\\tTrain Loss: {epoch_loss_avg:.3f}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["wandb"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["{'name': 'len_split_hyper_test',\n"," 'method': 'grid',\n"," 'parameters': {'dropout': {'values': [0.1]},\n","  'head': {'values': [6, 8, 10]},\n","  'layer': {'values': [1, 2]},\n","  'd_feedforward': {'values': [128, 256, 512]}},\n"," 'metric': {'name': 'loss', 'goal': 'minimize'}}"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["parameters_dict = {\n","    'dropout': {\n","          'values': [0.1]\n","        },\n","    'head': {\n","          'values': [6, 8, 10]\n","        },\n","    'layer': {\n","          'values': [1, 2]},\n","          \n","    'd_feedforward': {\n","          'values': [128, 256, 512]}\n","    }\n","\n","metric = {\n","    'name': 'loss',\n","    'goal': 'minimize'   \n","    }\n","\n","sweep_config = {\n","    'name': \"len_split_hyper_test\",\n","    'method': 'grid'\n","    }\n","\n","sweep_config['parameters'] = parameters_dict\n","sweep_config['metric'] = metric\n","\n","sweep_config"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Create sweep with ID: gulveqfe\n","Sweep URL: https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\n"]}],"source":["sweep_id = wandb.sweep(sweep_config, project=\"ATNLP\")"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["def train_sweep(config=None):\n","    with wandb.init(config=config):\n","        config = wandb.config\n","        args = Lang(dropout = config.dropout,\n","                    nhead = config.head,\n","                    d_model = config.head * 9,\n","                    num_encoder_layers = config.layer,\n","                    num_decoder_layers = config.layer,\n","                    d_feedforward = config.d_feedforward)\n","                    \n","        train(args)"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qhn8f2hf with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 6\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 1\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_152325-qhn8f2hf</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/qhn8f2hf\" target=\"_blank\">glamorous-sweep-1</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.249 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 1.037 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 1, 8, 1, 8]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.916 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 3, 6, 4, 1, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.908 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 6, 6]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.887 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 4, 4, 4]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.758 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.809 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 8, 3, 3, 3]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.790 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 5, 5, 8, 5, 5, 8, 5, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.701 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.776 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 6, 6, 6, 7, 6, 7, 1, 7, 6, 5, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.722 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 6, 8, 7, 8, 7, 7, 8, 7, 7, 8]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.628 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 4, 8, 8, 4, 8, 8, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.674 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.597 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 4, 6, 7, 8, 7, 8, 7, 6, 4, 6, 7, 8, 7, 8, 7, 6]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.657 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 7, 6, 7, 6, 3, 8, 3, 8, 3, 8, 7, 8, 3, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.586 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]) type: False\n","Epoch: 01 | Time: 2m 4s\n","\tTrain Loss: 0.779\n","Epoch: 1 | Iter: 999 | Loss: 0.917 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 5, 8, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.815 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 7, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.709 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 3, 6, 3, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.659 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 6, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.694 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.572 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.585 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([6, 3, 6, 3, 8, 3, 6, 3, 3, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.580 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 5, 8, 8, 5, 8, 8, 7, 1, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.560 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.633 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([5, 5, 6, 6, 7, 6, 7, 6, 7, 5, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.531 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 6, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.471 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 8, 8, 8, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.565 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 6, 3, 8, 3, 3, 8, 8, 8, 8, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.459 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([6, 7, 6, 4, 6, 4, 6, 7, 8, 4, 8, 7, 8, 7, 8, 4, 8]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.558 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.471 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]) type: False\n","Epoch: 02 | Time: 2m 5s\n","\tTrain Loss: 0.601\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec2eaa3b1b704a2688c40ff18a3f5b79","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.028 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.036908…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>█▇▅▃▆▅▃▅▆▄▄▂▅▃▃▃▃▃▅▃▅▄▅▅▄▆▃▃▂▅▃▂▃▂▂▁▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.60135</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.03674</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">glamorous-sweep-1</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/qhn8f2hf\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/qhn8f2hf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_152325-qhn8f2hf/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uf9tk9wd with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 6\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 2\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_152749-uf9tk9wd</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/uf9tk9wd\" target=\"_blank\">denim-sweep-2</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.084 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.872 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.784 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.765 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 5, 1, 6, 1]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.746 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([4, 8, 8, 4, 4, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.679 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.711 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 8, 6, 8, 3, 8, 8, 1, 3]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.689 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 8, 7, 8, 7, 7, 8, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.598 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 3, 3, 8, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.663 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 6, 6, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.611 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 8, 8, 8, 8, 8, 8, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.517 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 3, 8, 4, 8, 4, 8]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.519 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.445 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([6, 7, 6, 7, 6, 7, 6, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.527 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1, 3, 1]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.441 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 7, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 3m 44s\n","\tTrain Loss: 0.649\n","Epoch: 1 | Iter: 999 | Loss: 0.777 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.625 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.544 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.518 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.527 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.438 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.473 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([6, 3, 8, 3, 6, 6, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.436 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 8, 5, 8, 8, 7, 8, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.392 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 8, 8, 3, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.446 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([6, 5, 5, 6, 5, 6, 7, 6, 6, 6, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.363 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.353 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 4, 4, 8, 8, 8, 3, 8]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.379 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.292 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 8]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.351 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 7, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.298 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 7, 1]) type: False\n","Epoch: 02 | Time: 3m 43s\n","\tTrain Loss: 0.440\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▇▄▅▃█▅▃▆▅▄▄▂▄▅▂▃▃▄▃▃▆▃▃▃▃▆▃▂▆▃▄▂▂▂▃▁▃▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.43969</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.22906</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">denim-sweep-2</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/uf9tk9wd\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/uf9tk9wd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_152749-uf9tk9wd/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6gsq21qi with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 8\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 1\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_153541-6gsq21qi</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/6gsq21qi\" target=\"_blank\">radiant-sweep-3</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.183 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 1, 5, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 1.017 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.928 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 3, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.910 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 1, 6, 6]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.912 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 1, 8, 4, 8, 4, 8, 4, 8]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.806 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([6, 4, 8, 4, 6, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.823 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 8, 3, 3, 3]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.786 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 5, 5, 8, 5, 5, 8, 5, 5, 8, 8]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.673 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 8, 8, 3, 8, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.772 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 7, 6, 6, 7, 6, 7, 7, 5, 6, 7, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.720 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 8, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.662 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.703 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.612 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 4, 6, 4, 6, 4, 6, 4, 6, 4, 6, 7, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.679 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 6, 7, 8, 7, 6, 3, 8, 7, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.628 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 2m 5s\n","\tTrain Loss: 0.788\n","Epoch: 1 | Iter: 999 | Loss: 0.919 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 5, 1, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.823 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.702 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.694 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 1, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.732 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 4, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.633 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.634 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 8, 3, 6, 6, 6, 3, 3, 3]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.616 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 5, 5, 8, 8, 5, 8, 8, 7, 8, 7]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.575 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 8, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.674 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([6, 6, 6, 6, 5, 6, 7, 6, 5, 7, 5, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.573 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.519 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 3, 3, 8, 8, 4, 8, 8, 4, 8, 8, 1, 8, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.634 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.490 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 4, 6, 4, 6, 4, 6, 7, 8, 4, 8, 4, 8, 4, 1, 4, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.588 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.525 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 5, 8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 02 | Time: 2m 9s\n","\tTrain Loss: 0.637\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31be782051ee4d5dab13f156ff0964d3","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▆▇▄▃▆▆▄▆█▃▃▂▆█▃▅▃▄▅▄▇▄▆▆▅▄▅▄▂▃▃▃▄▄▄▁▃▅▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.63712</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.14383</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">radiant-sweep-3</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/6gsq21qi\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/6gsq21qi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_153541-6gsq21qi/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 318r0l2f with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 8\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 2\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_154019-318r0l2f</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/318r0l2f\" target=\"_blank\">charmed-sweep-4</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.024 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.842 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.761 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.756 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 5, 5, 6, 5, 1]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.735 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.686 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.687 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 8, 3, 6, 3, 1, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.643 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 7, 8, 8, 8, 7, 8, 7, 7, 7, 7]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.570 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.643 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([6, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.539 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.458 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 3, 8, 8, 8, 8, 8, 3, 8, 8, 1, 3, 8]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.491 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 3, 6, 3, 8, 8, 8, 8, 8, 8, 8, 3, 8, 1, 8]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.400 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([6, 7, 6, 4, 6, 7, 6, 7, 6, 7, 8, 7, 8, 7, 8, 7, 8]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.469 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 7, 6, 3, 6, 3, 6, 3, 8, 3, 6, 3, 8, 3, 8, 3, 1, 3, 1]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.391 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 3m 59s\n","\tTrain Loss: 0.615\n","Epoch: 1 | Iter: 999 | Loss: 0.734 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 3, 5, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.601 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.521 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 6]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.501 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.508 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([4, 4, 8, 4, 8, 4, 4, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.429 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 6, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.451 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 3, 8, 3, 8, 8, 1, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.427 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 7, 7, 8, 8, 7, 8, 7, 7, 7, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.403 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.412 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.345 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.330 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 8, 1, 4, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.342 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.293 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 1, 7, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.350 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.287 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 5, 5, 1]) type: False\n","Epoch: 02 | Time: 3m 52s\n","\tTrain Loss: 0.424\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fdeca49bae6f48c1b2e03adf80f5c337","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▅▄▃▂█▅▃▅▇▅▃▂▃▃▃▃▂▂▃▃▄▄▆▃▃▃▄▁▃▃▃▂▂▂▁▁▂▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.42357</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.00882</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">charmed-sweep-4</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/318r0l2f\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/318r0l2f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_154019-318r0l2f/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: py5jbhpl with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 10\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 1\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_154828-py5jbhpl</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/py5jbhpl\" target=\"_blank\">glowing-sweep-5</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.132 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.997 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.928 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 1, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.896 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 6, 6]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.908 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 8]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.765 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([6, 4, 8, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.827 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 8, 6, 8, 6, 8, 8, 3, 3]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.803 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 8, 8, 5, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.695 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.795 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 5, 5, 5, 7, 7, 7, 1, 7, 6, 7, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.734 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 6, 8, 7, 8, 7, 7, 8, 7, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.646 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 3, 8, 8, 8, 8, 4, 4, 8, 4, 8, 8, 8]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.685 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.603 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 6, 4, 6, 4, 1, 7, 6, 7, 6, 4, 8]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.681 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 7, 6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.640 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 2m 19s\n","\tTrain Loss: 0.784\n","Epoch: 1 | Iter: 999 | Loss: 0.914 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.854 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.717 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.664 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 5, 6, 1, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.715 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.597 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.632 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([6, 3, 6, 6, 6, 6, 6, 3, 1, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.656 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 8, 7, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.584 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.697 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([6, 5, 5, 5, 7, 6, 5, 6, 5, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.585 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.518 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.608 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 3, 3, 3, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.499 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 4, 8, 4, 1, 4, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.631 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 7, 6, 3, 6, 7, 6, 3, 8, 7, 8, 3, 8, 3, 8, 3, 1, 7, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.578 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8]) type: False\n","Epoch: 02 | Time: 2m 21s\n","\tTrain Loss: 0.645\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▇▇▄▂█▄▆▆▆▅▂▃▅▆▄▄▆▅▄▃▇▆▆▆█▄▅▅▂▃▄▂▃▃▂▁▄▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.64466</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.05762</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">glowing-sweep-5</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/py5jbhpl\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/py5jbhpl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_154828-py5jbhpl/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8fwh9c8k with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 128\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 10\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 2\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_155333-8fwh9c8k</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/8fwh9c8k\" target=\"_blank\">copper-sweep-6</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 0.990 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.849 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.760 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.757 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 5, 6, 6, 1, 1]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.748 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.661 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.696 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 6, 8, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.637 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 5, 8, 8, 7, 8, 7, 5, 1, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.555 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.630 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 7, 7, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.541 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.462 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 4, 4, 8, 8, 4, 8, 3, 4, 8, 3, 8, 3, 8]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.484 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.399 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 4, 8, 4, 8, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.460 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.388 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 4m 13s\n","\tTrain Loss: 0.609\n","Epoch: 1 | Iter: 999 | Loss: 0.711 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.593 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 7, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.506 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.498 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 5, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.500 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.403 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.459 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([6, 3, 6, 6, 1, 3, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.404 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 8, 5, 8, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.367 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 3, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.396 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 5, 6, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.349 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.330 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 8]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.344 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.308 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 4, 6, 4, 8, 4, 8, 7, 8, 7, 8, 7, 8, 7, 8]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.374 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.291 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 8, 5, 8, 5, 1]) type: False\n","Epoch: 02 | Time: 4m 15s\n","\tTrain Loss: 0.418\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▄▄▂▁▅▄▂▄▆▃▂▂▃▂▂▄▂▂▂▂▂▂▃▃▃▅▄▂▃▃▂▂▂▂▁▁▂█▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.41793</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.01424</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">copper-sweep-6</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/8fwh9c8k\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/8fwh9c8k</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_155333-8fwh9c8k/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: azosibd2 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 6\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 1\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_160228-azosibd2</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/azosibd2\" target=\"_blank\">bumbling-sweep-7</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.226 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 1.017 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.946 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 1, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.886 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 1, 6, 6]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.859 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 4, 4, 4]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.775 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.799 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 8, 6, 8, 3, 8, 3, 3, 3]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.778 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 5, 5, 8, 8, 5, 8, 5, 5, 8, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.679 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 3, 8, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.768 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([6, 5, 6, 6, 7, 1, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.685 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 6, 8, 7, 8]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.601 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 4, 8, 8, 4, 8, 8, 1, 4, 1]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.693 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.583 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 4, 8, 7, 6, 4, 6, 7, 6, 7, 8, 7, 8, 4, 8]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.669 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 6, 3, 1]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.604 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 2m 8s\n","\tTrain Loss: 0.772\n","Epoch: 1 | Iter: 999 | Loss: 0.897 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([5, 8, 8, 5, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.785 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.681 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.679 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.675 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.597 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 6, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.602 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 8, 3, 6, 3, 6, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.580 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 7, 8, 8, 5, 8, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.549 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.633 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 5, 5, 6, 5, 6, 5, 6, 5, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.564 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.502 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 3, 8, 4, 4, 8, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.581 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.465 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 7, 6, 7, 6, 4, 6, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.572 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 7, 6, 3, 8, 7, 8, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.487 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 1, 5, 1]) type: False\n","Epoch: 02 | Time: 2m 11s\n","\tTrain Loss: 0.605\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75df05d12f1647258bd201e145a5f245","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▇█▄▂█▃▂▆▅▅▃▂▅▂▃▃▃▄▃▃▆▄▃▄▅▃▄▃▂▃▄▂▃▂▃▁▃▅▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.60549</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.06491</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">bumbling-sweep-7</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/azosibd2\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/azosibd2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_160228-azosibd2/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5exks045 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 6\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 2\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_160711-5exks045</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/5exks045\" target=\"_blank\">absurd-sweep-8</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.061 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.884 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.764 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.748 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 5, 6, 6, 6, 1]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.722 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.658 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.694 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 8, 6, 8, 3, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.611 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 7, 8, 7, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.565 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.633 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([5, 5, 6, 6, 7, 6, 7, 6, 7, 6, 5, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.518 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.438 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 4, 4, 8, 8, 4, 8, 8, 4, 8, 8, 8, 3, 8]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.489 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.368 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([6, 7, 6, 7, 6, 7, 6, 7, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.455 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 6, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.363 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 1]) type: False\n","Epoch: 01 | Time: 4m 1s\n","\tTrain Loss: 0.605\n","Epoch: 1 | Iter: 999 | Loss: 0.696 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 5, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.584 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 7, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.505 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.466 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.484 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.419 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.432 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 3, 6, 3, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.399 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 8, 8, 8, 7, 1, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.365 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.390 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.338 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.307 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 3, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.343 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.270 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.312 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.264 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 02 | Time: 3m 46s\n","\tTrain Loss: 0.400\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6423d4ab62d14dbcb3a8f15d83908f8f","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.076790…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>█▆▂▂█▄▃▅▇▄▃▃▄▄▃▂▂▂▅▃▄▄▄▃▃▃▃▂▃▄▃▂▄▁▁▁▂▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.39995</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.02154</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">absurd-sweep-8</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/5exks045\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/5exks045</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_160711-5exks045/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zlaowr40 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 8\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 1\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_161516-zlaowr40</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/zlaowr40\" target=\"_blank\">neat-sweep-9</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.158 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.993 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.881 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 6]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.874 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 5, 5, 6, 6, 6, 5, 6]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.848 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 4, 4, 8]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.744 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.792 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 8, 6, 8, 6, 8, 8, 3, 3]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.759 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 5, 8, 5, 7, 8, 8, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.654 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.787 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([6, 5, 5, 6, 5, 6, 7, 6, 7, 6, 5, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.715 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 8, 8, 7, 8, 8, 7, 6, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.618 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.682 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.565 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 4, 6, 4, 6, 4, 6, 4, 6, 4, 8, 4, 6, 4, 1, 4, 6]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.669 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 6, 3, 8, 3, 6, 3, 6, 3, 6, 7, 6, 3, 1, 3, 6]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.602 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 5, 8, 7, 8, 7, 8, 7, 8, 5, 8, 7, 8, 7, 8]) type: False\n","Epoch: 01 | Time: 2m 17s\n","\tTrain Loss: 0.759\n","Epoch: 1 | Iter: 999 | Loss: 0.881 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 5, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.714 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.656 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 3, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.637 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.689 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.586 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.613 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 8, 6, 8, 6, 6, 3, 3, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.597 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 5, 8, 8, 7, 8, 8, 5, 8, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.554 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.649 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 5, 6, 6, 5, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.539 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 8, 8, 7, 6, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.471 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 8, 8, 8, 8, 3, 8]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.560 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 3, 8, 3, 8, 8, 8, 3, 8, 8, 8, 8, 3]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.433 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 4, 6, 4, 6, 4, 6, 4, 6, 7, 8, 7, 6, 7, 1, 7, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.565 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 7, 6, 7, 6, 7, 6, 7, 8, 7, 8, 7, 6, 7, 6, 7, 8, 3, 6]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.478 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8]) type: False\n","Epoch: 02 | Time: 2m 16s\n","\tTrain Loss: 0.592\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8718bfbc84dc4bd9a6094ace37a00f23","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.028 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.036868…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▇█▄▂█▅▂▆▆▄▂▂▅▄▃▄▃▆▅▄▄▄▄▃▄▃▅▆▄▂▃▃▃▁▃▁▂▅▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.59164</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.0425</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">neat-sweep-9</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/zlaowr40\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/zlaowr40</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_161516-zlaowr40/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n7hv4hop with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 8\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 2\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_162005-n7hv4hop</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/n7hv4hop\" target=\"_blank\">ruby-sweep-10</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.028 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.865 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.754 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.749 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 6, 1]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.719 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.658 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.686 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 8, 3, 8, 6, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.632 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 7, 7, 8, 8, 5, 8, 7, 7, 7, 7]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.559 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 3, 8, 3, 3, 8]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.637 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([5, 7, 6, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.522 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.445 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 3, 8, 8, 8, 8, 8, 8, 8, 3, 1, 3, 1]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.452 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.378 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.464 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.385 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 4m 11s\n","\tTrain Loss: 0.604\n","Epoch: 1 | Iter: 999 | Loss: 0.773 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([5, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.611 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 7, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.530 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.502 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 5, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.495 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.401 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([6, 4, 6, 8, 8, 8, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.433 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([6, 3, 8, 3, 8, 3, 8, 3, 8, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.415 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 8, 5, 8, 8, 7, 7, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.363 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 8, 3, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.355 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([5, 5, 6, 6, 7, 6, 7, 1, 7, 6, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.335 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.309 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 1, 3, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.319 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.280 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.332 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 7, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 6]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.264 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 8, 1, 7, 1]) type: False\n","Epoch: 02 | Time: 4m 10s\n","\tTrain Loss: 0.409\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▅▅▂▁█▃▂▅▇▃▂▂▅▂▃▃▂▂▂▂▇▄▄▃▄▂▂▂▂▄▂▂▂▁▁▁▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.40875</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.01329</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">ruby-sweep-10</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/n7hv4hop\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/n7hv4hop</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_162005-n7hv4hop/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ihqr50kr with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 10\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 1\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_162845-ihqr50kr</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/ihqr50kr\" target=\"_blank\">lucky-sweep-11</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.134 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.983 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.919 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 6]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.888 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 5, 5, 5, 6, 6, 6]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.856 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 1, 4, 1]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.768 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 6, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.804 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 8, 6, 8, 3, 6, 3, 3, 3]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.754 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 5, 8, 5, 5, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.672 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.769 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([6, 6, 6, 6, 5, 6, 7, 6, 5, 6, 5, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.720 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 6, 8, 7, 6, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.620 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 3, 8, 8, 4, 8, 4, 8, 4, 8]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.695 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.603 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 7, 6, 4, 8, 4, 6, 4, 6, 7, 8, 7, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.673 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 6, 3, 6, 3, 8, 3, 6, 3, 6, 3, 6, 3, 6, 3, 1]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.608 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 8, 7, 8, 5, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 2m 22s\n","\tTrain Loss: 0.768\n","Epoch: 1 | Iter: 999 | Loss: 0.885 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 5, 5, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.818 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.729 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 1, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.678 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 5, 6, 6, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.690 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.585 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.616 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 8, 3, 3, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.590 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 5, 7, 8, 8, 5, 8, 8, 5, 5, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.545 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.659 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([6, 6, 6, 6, 5, 6, 5, 6, 5, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.586 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.507 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 1, 4, 8, 4, 8]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.579 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.498 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 4, 6, 7, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 1, 4, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.582 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 7, 8, 3, 8, 3, 8, 7, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.512 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8]) type: False\n","Epoch: 02 | Time: 2m 23s\n","\tTrain Loss: 0.619\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▃▄▂▂▄▄▂▃▃▂▁▁▃▃▂▂▂▃▃▂█▃▂▃▅▅▃▁▃▂▂▂▂▂▁▂▂▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.61944</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.02765</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">lucky-sweep-11</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/ihqr50kr\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/ihqr50kr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_162845-ihqr50kr/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l59ws02y with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 256\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 10\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 2\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_163349-l59ws02y</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/l59ws02y\" target=\"_blank\">effortless-sweep-12</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 0.990 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.845 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.762 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.744 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 5, 5, 6, 6, 6]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.732 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([4, 8, 8, 4, 8, 4, 8, 4, 4]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.672 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([6, 4, 8, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.676 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 6, 6, 8, 3, 8, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.622 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 8, 7, 8, 7, 7, 8, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.550 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.591 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 5, 6, 6, 7, 6, 7, 6, 7, 6, 7, 6]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.503 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 8, 8, 8, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.435 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 3, 4, 8, 8, 4, 8, 4, 3, 8, 3, 1, 3, 1]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.446 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 8, 8, 8, 3, 3, 8, 8, 3, 8, 8, 8, 1, 8]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.388 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 7, 6, 4, 6, 7, 8, 7, 8, 7, 1, 7, 1, 7, 1]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.438 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.360 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 4m 40s\n","\tTrain Loss: 0.592\n","Epoch: 1 | Iter: 999 | Loss: 0.710 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.609 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 1, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.511 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.490 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 5, 6, 5, 6, 5, 5, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.484 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.427 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 6, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.447 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 6, 3, 6, 3, 8, 8, 3, 3]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.414 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 8, 7, 8, 8, 7, 7, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.383 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 8, 3, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.395 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.344 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.318 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.335 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.277 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.348 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.276 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 1]) type: False\n","Epoch: 02 | Time: 4m 25s\n","\tTrain Loss: 0.412\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▇█▂▃█▅▃▅▅▃▃▃▅▃▃▃▃▂▂▂▄▂▅▄▄▃▇▃▂▃▃▂▃▂▁▁▁▅▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.41235</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.03713</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">effortless-sweep-12</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/l59ws02y\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/l59ws02y</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_163349-l59ws02y/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 83xp6gxd with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 6\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 1\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_164311-83xp6gxd</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/83xp6gxd\" target=\"_blank\">astral-sweep-13</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.166 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.965 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.882 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.846 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 5, 5, 6, 6, 1, 5, 1]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.848 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.751 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([6, 4, 8, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.799 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 6, 6, 8, 3, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.732 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 7, 7, 8, 8, 5, 8, 7, 7, 1, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.643 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.763 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 5, 5, 5, 7, 6, 7, 6, 5, 6, 5, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.699 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 6, 8, 8, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.629 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 4, 4, 8, 8, 8, 8, 8, 8, 8, 4, 1, 8, 1]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.655 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 8, 3, 3, 3, 3, 3, 8, 8, 3, 3, 8, 3, 3]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.579 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 4, 6, 7, 8, 7, 6, 7, 8, 7, 6, 4, 1, 7, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.618 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 7, 6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1, 3, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.557 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 2m 12s\n","\tTrain Loss: 0.743\n","Epoch: 1 | Iter: 999 | Loss: 0.833 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([5, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.701 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 7, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.634 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.620 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 6, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.668 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 4, 4, 8, 4, 4, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.548 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 6, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.582 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 6, 3, 8, 3, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.551 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 8, 5, 8, 7, 7, 7, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.494 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 8, 3, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.580 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 5, 5, 6, 5, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.498 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.436 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 8, 8, 4, 4, 8, 4, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.472 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.407 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 7, 6, 7, 8, 7, 8, 7, 8, 7, 1, 7, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.495 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.419 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 02 | Time: 2m 13s\n","\tTrain Loss: 0.549\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf350ddf72fe4f1686899cf402897d45","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.028 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.036870…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>█▇▂▂▆▅▂▅▆▄▂▂▄▃▃▄▂▃▅▄▃▄▄▇▄▆▃▁▂▃▃▂▂▃▁▁▂▄▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.54882</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.03324</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">astral-sweep-13</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/83xp6gxd\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/83xp6gxd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_164311-83xp6gxd/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ex782oov with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 6\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 2\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_164755-ex782oov</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/ex782oov\" target=\"_blank\">fluent-sweep-14</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.090 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.886 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.776 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.740 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 1, 1]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.724 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.649 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 6, 4, 8, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.669 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([6, 3, 8, 3, 8, 3, 8, 3, 1, 1]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.620 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 5, 8, 8, 7, 8, 8, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.564 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.632 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 7, 7, 7, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.525 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 6, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.477 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 3, 8, 4, 8, 4, 8]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.492 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.429 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 4, 6, 4, 6, 7, 6, 7, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.500 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 3, 6, 3, 6, 3, 8, 3, 6, 3, 6, 3, 6, 3, 6, 3, 1, 3, 1]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.403 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 5m 2s\n","\tTrain Loss: 0.619\n","Epoch: 1 | Iter: 999 | Loss: 0.728 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 5, 3, 3, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.569 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 7, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.482 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.477 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.478 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.379 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 6, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.432 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 3, 6, 3, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.421 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 5, 8, 8, 5, 8, 5, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.392 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 8, 3, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.385 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([5, 5, 6, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.331 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.318 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 3, 8, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.345 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.282 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([6, 7, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.341 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 7, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.245 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 1, 7, 8, 5, 8, 5, 1]) type: False\n","Epoch: 02 | Time: 4m 56s\n","\tTrain Loss: 0.402\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0908b3dca35542feb6ba476f48fe6025","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▆█▄▁▆▅▃▂▇▄▂▁▃▁▂▃▂▄▂▃▄▂▃▅▃▅▂▃▃▄▃▄▂▂▁▁▁▃▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.40167</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.0122</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">fluent-sweep-14</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/ex782oov\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/ex782oov</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_164755-ex782oov/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sznpz97n with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 8\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 1\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_165817-sznpz97n</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/sznpz97n\" target=\"_blank\">feasible-sweep-15</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.167 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 3, 8, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.988 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.904 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 1, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.872 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 5, 6, 6, 6, 6, 6, 1]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.875 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 1, 4, 1]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.740 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.808 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.752 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 5, 8, 8, 5, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.629 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.771 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 5, 5, 6, 7, 6, 7, 1, 7, 6, 5, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.697 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.609 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 3, 8, 8, 8, 8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.695 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.575 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 8, 7, 6, 7, 6, 7, 8, 7, 8]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.677 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 7, 6, 7, 8, 7, 6, 7, 6, 7, 8, 7, 1, 7, 6, 3, 1, 7, 6]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.627 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 2m 38s\n","\tTrain Loss: 0.763\n","Epoch: 1 | Iter: 999 | Loss: 0.853 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 5, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.788 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 7, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.703 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 3, 6]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.655 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 5, 6, 6, 6, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.694 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.597 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 6, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.618 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 8, 3, 8, 3, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.593 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 8, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.557 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.669 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 5, 5, 6, 5, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.571 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 1, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.504 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 8, 8, 8, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.563 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.437 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 4, 6, 4, 6, 7, 6, 7, 6, 7, 6, 7, 8, 7, 8, 7, 8]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.576 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.505 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 1, 7, 8]) type: False\n","Epoch: 02 | Time: 2m 31s\n","\tTrain Loss: 0.610\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eaf336f3a54540bf9a0895447c0c5f1c","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▅▅▂▂█▄▂▃▅▃▂▂▃▃▃▂▂▅▃▃▅▃▃▆▃▃▃▂▂▂▂▂▂▂▂▁▂▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.61002</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.09851</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">feasible-sweep-15</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/sznpz97n\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/sznpz97n</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_165817-sznpz97n/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: liing6my with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 8\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 2\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_170342-liing6my</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/liing6my\" target=\"_blank\">still-sweep-16</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.034 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.857 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.773 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.739 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 5, 5, 6, 5, 1]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.721 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 4, 4, 4]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.681 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.676 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 8, 6, 6, 3, 6, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.636 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 7, 7, 7, 7, 7, 1, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.568 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 3, 8, 3, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.630 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 7, 7, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.542 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 6, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.453 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 3, 8, 8, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.487 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 8, 8, 8, 8, 8, 8, 1, 8, 8, 8, 1, 8]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.404 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 7, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.473 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 3, 6, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.399 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8]) type: False\n","Epoch: 01 | Time: 4m 36s\n","\tTrain Loss: 0.613\n","Epoch: 1 | Iter: 999 | Loss: 0.787 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.586 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 7, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.517 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.501 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.474 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.395 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.398 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 6, 3, 6, 3, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.410 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 8, 8, 8, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.390 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.400 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.349 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.332 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.338 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.277 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([6, 7, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.325 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 7, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.275 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 1, 5, 1]) type: False\n","Epoch: 02 | Time: 4m 47s\n","\tTrain Loss: 0.412\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a660a532411d45639faac5e5f8b90c16","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▆▅▂▁▇▄▃▆▅▄▃▂▃▃▃▃▄▂▃▂█▄▅▆▃▂▃▂▂▃▃▃▃▃▂▁▁▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.41179</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.05188</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">still-sweep-16</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/liing6my\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/liing6my</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_170342-liing6my/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: adb3mrty with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 10\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 1\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_171320-adb3mrty</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/adb3mrty\" target=\"_blank\">olive-sweep-17</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.136 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 5, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.980 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 1, 8]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.887 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.817 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 5, 6, 5, 6, 6, 5, 6]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.865 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.742 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 8, 4, 6, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.792 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 8, 3, 8, 6, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.773 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 7, 5, 8, 8, 7, 8, 5, 5, 1, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.667 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 3, 8, 8, 8, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.784 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([6, 7, 6, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.681 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([8, 6, 7, 8, 8, 7, 8, 8, 8, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.604 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 3, 8, 4, 3, 8, 8, 3, 8, 8, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.705 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.580 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 7, 6, 4, 6, 4, 6, 4, 6, 7, 6, 4, 6]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.679 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 8, 3, 6, 3, 6, 3, 6, 3, 6, 3, 8, 3, 8, 3, 6]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.631 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]) type: False\n","Epoch: 01 | Time: 2m 37s\n","\tTrain Loss: 0.759\n","Epoch: 1 | Iter: 999 | Loss: 0.870 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 5, 8, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.749 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 7, 7, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.657 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.631 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 5, 6, 6, 1, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.708 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.592 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.632 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 8, 6, 8, 3, 8, 8, 1, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.605 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 8, 5, 8, 8, 7, 1, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.561 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.643 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([6, 6, 6, 6, 7, 6, 7, 6, 5, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.536 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 6, 8, 7, 6, 8, 7, 6, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.489 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 3, 8, 8, 4, 8, 4, 4, 8, 8, 1, 3, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.578 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.464 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 4, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.582 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 6, 3, 1, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.492 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 1]) type: False\n","Epoch: 02 | Time: 2m 43s\n","\tTrain Loss: 0.602\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"50311b806b574ad69ecba2171b6d8919","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.028 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.036870…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>▇▅▃▁▆▄▂█▆▄▁▂▅▃▄▄▄▄▅▃▅▇▃▅▅▂▄▃▂▄▄▅▃▃▃▂▂▃▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.60233</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.11564</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">olive-sweep-17</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/adb3mrty\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/adb3mrty</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_171320-adb3mrty/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sj3kzxq6 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \td_feedforward: 512\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: \thead: 10\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer: 2\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["wandb version 0.13.7 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/wandb/run-20221228_171856-sj3kzxq6</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/sj3kzxq6\" target=\"_blank\">solar-sweep-18</a></strong> to <a href=\"https://wandb.ai/yuqinzhou/ATNLP\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/sweeps/gulveqfe</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 0 | Iter: 999 | Loss: 1.024 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 1999 | Loss: 0.855 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 0 | Iter: 2999 | Loss: 0.735 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 6, 6, 4, 6, 4, 1]) type: False\n","Epoch: 0 | Iter: 3999 | Loss: 0.728 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 6, 6, 5, 6]) type: False\n","Epoch: 0 | Iter: 4999 | Loss: 0.710 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 5999 | Loss: 0.640 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 0 | Iter: 6999 | Loss: 0.647 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([6, 3, 8, 3, 6, 3, 8, 7, 1, 1]) type: False\n","Epoch: 0 | Iter: 7999 | Loss: 0.592 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 8, 7, 7, 8, 7, 1, 1]) type: False\n","Epoch: 0 | Iter: 8999 | Loss: 0.534 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 3, 3, 1]) type: False\n","Epoch: 0 | Iter: 9999 | Loss: 0.617 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([6, 5, 7, 6, 7, 6, 7, 6, 7, 6, 7, 1]) type: False\n","Epoch: 0 | Iter: 10999 | Loss: 0.498 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 6, 8, 7, 8, 8, 7, 8, 8, 7, 1]) type: False\n","Epoch: 0 | Iter: 11999 | Loss: 0.423 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 3, 3, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 8]) type: False\n","Epoch: 0 | Iter: 12999 | Loss: 0.438 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 1]) type: False\n","Epoch: 0 | Iter: 13999 | Loss: 0.369 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 8]) type: False\n","Epoch: 0 | Iter: 14999 | Loss: 0.423 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([6, 3, 6, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1, 3, 1]) type: False\n","Epoch: 0 | Iter: 15999 | Loss: 0.347 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8]) type: False\n","Epoch: 01 | Time: 5m 56s\n","\tTrain Loss: 0.581\n","Epoch: 1 | Iter: 999 | Loss: 0.725 \n"," Target tensor([5, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 5, 1]) type: False\n","Epoch: 1 | Iter: 1999 | Loss: 0.580 \n"," Target tensor([7, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 7, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 2999 | Loss: 0.487 \n"," Target tensor([6, 3, 6, 3, 6, 4, 1]) \n"," Predict tensor([6, 4, 6, 4, 6, 4, 1]) type: False\n","Epoch: 1 | Iter: 3999 | Loss: 0.463 \n"," Target tensor([6, 6, 6, 6, 5, 6, 5, 1]) \n"," Predict tensor([6, 6, 6, 6, 5, 6, 5, 1]) type: False\n","Epoch: 1 | Iter: 4999 | Loss: 0.478 \n"," Target tensor([4, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 8, 8, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 5999 | Loss: 0.376 \n"," Target tensor([6, 4, 8, 4, 8, 4, 8, 4, 1]) \n"," Predict tensor([8, 4, 6, 4, 8, 4, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 6999 | Loss: 0.411 \n"," Target tensor([6, 3, 6, 3, 6, 3, 8, 8, 8, 1]) \n"," Predict tensor([8, 3, 6, 3, 6, 3, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 7999 | Loss: 0.389 \n"," Target tensor([8, 8, 5, 8, 8, 5, 8, 8, 5, 7, 1]) \n"," Predict tensor([8, 8, 7, 8, 8, 5, 8, 7, 7, 8, 1]) type: False\n","Epoch: 1 | Iter: 8999 | Loss: 0.359 \n"," Target tensor([8, 3, 8, 3, 8, 8, 3, 8, 8, 3, 1]) \n"," Predict tensor([8, 8, 8, 3, 8, 3, 3, 8, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 9999 | Loss: 0.390 \n"," Target tensor([5, 5, 5, 6, 7, 6, 7, 6, 7, 6, 7, 1]) \n"," Predict tensor([7, 5, 5, 6, 7, 6, 7, 6, 7, 7, 7, 1]) type: False\n","Epoch: 1 | Iter: 10999 | Loss: 0.333 \n"," Target tensor([6, 6, 7, 8, 8, 7, 8, 8, 7, 8, 8, 7, 1]) \n"," Predict tensor([6, 6, 7, 6, 8, 7, 8, 7, 7, 8, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 11999 | Loss: 0.311 \n"," Target tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 8, 4, 8, 8, 4, 8, 8, 4, 8, 3, 8, 4, 1]) type: False\n","Epoch: 1 | Iter: 12999 | Loss: 0.357 \n"," Target tensor([6, 3, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) \n"," Predict tensor([8, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1]) type: False\n","Epoch: 1 | Iter: 13999 | Loss: 0.291 \n"," Target tensor([6, 4, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) \n"," Predict tensor([8, 7, 6, 4, 6, 4, 6, 4, 8, 7, 8, 7, 8, 7, 8, 7, 1]) type: False\n","Epoch: 1 | Iter: 14999 | Loss: 0.326 \n"," Target tensor([6, 7, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) \n"," Predict tensor([8, 3, 6, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 8, 3, 1]) type: False\n","Epoch: 1 | Iter: 15999 | Loss: 0.254 \n"," Target tensor([8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) \n"," Predict tensor([8, 5, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 5, 8, 5, 1]) type: False\n","Epoch: 02 | Time: 4m 54s\n","\tTrain Loss: 0.398\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04b518930c4048ccb6501915b4abedac","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/epoch_loss</td><td>█▁</td></tr><tr><td>train/iter</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██▁▁▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train/loss</td><td>█▅▄▁▇▃▂▇▅▃▂▂▄▂▂▃▂▃▂▂▂▃▄▄▃▄▄▂▁▅▂▃▃▂▃▁▂▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.3983</td></tr><tr><td>train/iter</td><td>16989</td></tr><tr><td>train/loss</td><td>0.13809</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">solar-sweep-18</strong>: <a href=\"https://wandb.ai/yuqinzhou/ATNLP/runs/sj3kzxq6\" target=\"_blank\">https://wandb.ai/yuqinzhou/ATNLP/runs/sj3kzxq6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221228_171856-sj3kzxq6/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"]}],"source":["wandb.agent(sweep_id, train_sweep)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Evaluation"]},{"cell_type":"code","execution_count":206,"metadata":{},"outputs":[],"source":["torch.save(model.state_dict(), 'simple-10.pt')"]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":207,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(torch.load('simple-10.pt'))"]},{"cell_type":"code","execution_count":208,"metadata":{},"outputs":[],"source":["def translate_sentence(batch, model, max_len = 50):\n","    model.eval()\n","    src = batch[0].T\n","    src_mask = model.make_src_mask(src)\n","    with torch.no_grad():\n","        enc_src = model.encoder(src, src_mask)\n","\n","    #\n","    trg_indexes = [SOS_token]\n","    for i in range(max_len):\n","        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n","        trg_mask = model.make_trg_mask(trg_tensor)\n","        with torch.no_grad():\n","            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n","        \n","        output = output[0][i]\n","        pred_token = output.argmax(0).item()\n","        trg_indexes.append(pred_token)\n","        #\n","        if pred_token == EOS_token:\n","            break\n","    return trg_indexes[1:], attention\n","\n","def test_accuracy(data, model):\n","    all_correct_trials = 0\n","    for i, batch in enumerate(data):\n","        trg = batch[1].T\n","        trg_out = trg[:,1:] ##[y_1,..., y_2, <EOS>]\n","        index, _  = translate_sentence(batch, model)\n","\n","        correct = trg_out[0].tolist() == index\n","        all_correct_trials += correct\n","        \n","        if (i+1)  % 100 == 0:\n","            print(i, all_correct_trials/ i)\n","            \n","    return all_correct_trials / len(data)"]},{"cell_type":"code","execution_count":209,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":210,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["99 0.7676767676767676\n","199 0.7386934673366834\n","299 0.7658862876254181\n","399 0.7719298245614035\n","499 0.7775551102204409\n","599 0.7779632721202003\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb Cell 31\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb#Y121sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_accuracy(test_pairs)\n","\u001b[1;32m/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb Cell 31\u001b[0m in \u001b[0;36mtest_accuracy\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb#Y121sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m trg \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mT\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb#Y121sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m trg_out \u001b[39m=\u001b[39m trg[:,\u001b[39m1\u001b[39m:] \u001b[39m##[y_1,..., y_2, <EOS>]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb#Y121sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m index, _  \u001b[39m=\u001b[39m translate_sentence(batch)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb#Y121sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m correct \u001b[39m=\u001b[39m trg_out[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist() \u001b[39m==\u001b[39m index\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb#Y121sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m all_correct_trials \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m correct\n","\u001b[1;32m/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb Cell 31\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(batch, max_len)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb#Y121sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m trg_mask \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmake_trg_mask(trg_tensor)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb#Y121sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb#Y121sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     output, attention \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecoder(trg_tensor, enc_src, trg_mask, src_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb#Y121sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m output \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m][i]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zhouyuqin/Desktop/ATNLP/transformer_scan/ATNLP_transformer_new.ipynb#Y121sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m pred_token \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39margmax(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mitem()\n","File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Desktop/ATNLP/transformer_scan/models/transformer_new.py:245\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, trg, enc_src, trg_mask, src_mask)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39m#trg = [batch size, trg len, hid dim]\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 245\u001b[0m     trg, attention \u001b[39m=\u001b[39m layer(trg, enc_src, trg_mask, src_mask)\n\u001b[1;32m    247\u001b[0m \u001b[39m#trg = [batch size, trg len, hid dim]\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39m#attention = [batch size, n heads, trg len, src len]\u001b[39;00m\n\u001b[1;32m    250\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_out(trg)\n","File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Desktop/ATNLP/transformer_scan/models/transformer_new.py:284\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, trg, enc_src, trg_mask, src_mask)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, trg, enc_src, trg_mask, src_mask):\n\u001b[1;32m    277\u001b[0m     \n\u001b[1;32m    278\u001b[0m     \u001b[39m#trg = [batch size, trg len, hid dim]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \n\u001b[1;32m    283\u001b[0m     \u001b[39m#self attention\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     _trg, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attention(trg, trg, trg, trg_mask)\n\u001b[1;32m    286\u001b[0m     \u001b[39m#dropout, residual connection and layer norm\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     trg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm(trg \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(_trg))\n","File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/Desktop/ATNLP/transformer_scan/models/transformer_new.py:143\u001b[0m, in \u001b[0;36mMultiHeadAttentionLayer.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    137\u001b[0m V \u001b[39m=\u001b[39m V\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39m#Q = [batch size, n heads, query len, head dim]\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39m#K = [batch size, n heads, key len, head dim]\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m#V = [batch size, n heads, value len, head dim]\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m energy \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(Q, K\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m2\u001b[39;49m)) \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[1;32m    145\u001b[0m \u001b[39m#energy = [batch size, n heads, query len, key len]\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["test_accuracy(test_pairs, model)"]},{"cell_type":"code","execution_count":213,"metadata":{},"outputs":[{"data":{"text/plain":["0.9854706749004734"]},"execution_count":213,"metadata":{},"output_type":"execute_result"}],"source":["def test_teacher(data, model):\n","    model.eval()\n","    with torch.no_grad():\n","        all_correct_trials = [] # list of booleans indicating whether correct\n","        \n","        for batch in data:\n","            src = batch[0].T\n","            trg = batch[1].T\n","            out, _ = model(src, trg[:,:-1])\n","\n","            preds = torch.argmax(out, dim = 2)\n","            correct_pred = preds == trg[:,1:]\n","\n","            correct_pred = correct_pred.cpu().numpy()\n","            correct = correct_pred.all(0).tolist()\n","            all_correct_trials += correct\n","\n","    accuracy = np.mean(all_correct_trials)\n","    model.train()\n","    return accuracy\n","\n","test_teacher(test_pairs, model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_teacher(run, args):\n","    model = Seq2Seq(enc, dec, pad_idx, pad_idx, device).to(device)\n","    model.train()\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr = args.learning_rate)\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for epoch in range(args.num_epochs):\n","        start_time = time.time()\n","        epoch_loss = 0\n","\n","        for iter, batch in enumerate(training_pairs):\n","            src = batch[0].T\n","            trg = batch[1].T\n","            \n","            optimizer.zero_grad()\n","            \n","            output, _ = model(src, trg[:,:-1]) ##[<SOS>, y_1, y_2]\n","                    \n","            #output = [batch size, trg len - 1, output dim]\n","            #trg = [batch size, trg len]\n","                \n","            output_dim = output.shape[-1]\n","                \n","            output = output.contiguous().view(-1, output_dim)\n","            trg = trg[:,1:].contiguous().view(-1) ##[y_1, y_2, <EOS>]\n","                    \n","            #output = [batch size * trg len - 1, output dim]\n","            #trg = [batch size * trg len - 1]\n","                \n","            loss = criterion(output, trg)\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","            # Record loss\n","            if iter % args.record_loss_every == 0:\n","                loss_datapoint = loss.data.item()\n","                print('Run:', run,\n","                        'Iter:', iter,\n","                        'Loss:', loss_datapoint)\n","\n","        end_time = time.time()\n","        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","        test_acc = test_teacher(model, test_pairs, device)\n","\n","        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'\\tTrain Loss: {epoch_loss / len(training_pairs):.3f} | Test accurac （teacher forcing): {test_acc}')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPn0HgDj9chBaWr6WYfBnce","collapsed_sections":["y5g7MErG4-4v"],"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13 (default, Mar 28 2022, 06:16:26) \n[Clang 12.0.0 ]"},"vscode":{"interpreter":{"hash":"81c0a520ab9d5f38718f11fc8e39c528dfdc4be4c6d24f2eb9940d412ba4096a"}}},"nbformat":4,"nbformat_minor":0}
